---
date: 2023-03-19T13:47:27.000Z
layout: post
title: 1-4. NMF
katex: True
subtitle: 'Advanced Mathematics for AI'
description: >-
  Advanced Mathematics for AI (Week 1)
image: >-
  https://velog.velcdn.com/images/dnr6054/post/3906995b-70f8-4c71-a97d-851b0eb5cca2/image.png
optimized_image: >-
  https://velog.velcdn.com/images/dnr6054/post/3906995b-70f8-4c71-a97d-851b0eb5cca2/image.png
category: math
tags:
  - NMF
  - 비음분해
author: i4song
paginate: true
---

> 본 글은 K-MOOC의 [인공지능 수학 고급(Advanced Mathematics for AI)](http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_60+2023_T1/course/) 강의를 듣고 요약한 글입니다. 

## 자연속의 데이터들

자연의 데이터는 대부분 음이 아닌 수 (0 또는 양수)로 주어진다. 

- Pixel Intensity
- Occurrence counts
- Food consumption
- User scores
- Stock market values

이를 분석할 때 우리가 음수가 아니라는 조건을 이용한다면 이를 좀 더 잘 분석할 수 있을 것이다.

이런 가정을 통해 행렬분해를 진행하는 것이 비음 분해(Nonnegative Matrix Factorization, **NMF**)가 된다. 

## Nonnegative Matrix Factorization

- $$A$$: $$F\times N$$ 행렬
  - $$N$$ 개의 문서, 예시, 결과
  - $$F$$ 개의 단어, 특성
  - $$a_n$$: $$n$$번째 열 벡터

- $$W$$: $$F\times K$$ 행렬
- $$H$$: $$K\times N$$ 행렬

이 때 우리는 다음과 같은 방법을 통해 NMF의 추정값$$A \approx WH = \hat{A}$$을 구할 수 있다.

$$
min\;D(A|\hat{A}) = min \sum_{f=1}^{F}\sum_{n=1}^{N}d(a_{fn}|\hat{a}_{fn})
$$

이 때 거리 함수 $$d$$는 다음 조건을 만족해야 한다.
- $$d(x|y)$$ 는 연속적이어야 한다.
- $$x,y\ge 0$$에 대해 $$d(x|y) \ge 0$$ 이어야 한다.
- $$x=y$$ 라면 $$d(x|y) = 0$$ 이어야 한다.

다음은 유명한 거리함수들이다. 

- Euclidean distance
  $$d(x, y) = (x-y)^2$$
- Kullback-Leibler divergence
  $$d(x, y) = x\log\frac x y-x+y$$
- Itakura-Saito divergence
  $$d(x, y) = \frac{x}{y} - \log\frac{x}{y} - 1$$
  
## Weighted Nonnegative Matrix Factorization

행렬의 위치에 따라 그 가중치를 다르게 하고 싶을 때가 있다.

어떤 영역을 더 **강조**하고 싶다거나 어떤 영역을 ~~배제~~하고 싶다던가 할 때, 다음과 같은 가중치를 통해서 **NMF**를 수행할 수도 있다.

$$
D(A|\hat{A}) = \sum_{f=1}^{F}\sum_{n=1}^{N}b_{fn}d(a_{fn}|\hat{a}_{fn})
$$